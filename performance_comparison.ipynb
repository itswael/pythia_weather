{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37dc697",
   "metadata": {},
   "source": [
    "# CHIRPS V3 Download Performance Comparison\n",
    "\n",
    "Comparing sequential vs multithreaded download performance for CHIRPS V3 data (366 daily files for year 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c4a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03747f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHIRPS_V3_BASE_URL = 'https://data.chc.ucsb.edu/products/CHIRPS/v3.0/daily/final/rnl'\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2020-12-31'\n",
    "DATA_DIR_SEQUENTIAL = './chirps_v3_sequential'\n",
    "DATA_DIR_MULTITHREADED = './chirps_v3_multithreaded'\n",
    "MAX_WORKERS = 10  # Number of concurrent download threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f12916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to download: 366\n",
      "Date range: 2020-01-01 to 2020-12-31\n"
     ]
    }
   ],
   "source": [
    "def generate_date_list(start_date, end_date):\n",
    "    \"\"\"Generate list of dates for download.\"\"\"\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    \n",
    "    dates = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        dates.append(current)\n",
    "        current += pd.Timedelta(days=1)\n",
    "    \n",
    "    return dates\n",
    "\n",
    "# Generate full year date list\n",
    "all_dates = generate_date_list(START_DATE, END_DATE)\n",
    "print(f\"Total files to download: {len(all_dates)}\")\n",
    "print(f\"Date range: {all_dates[0].date()} to {all_dates[-1].date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737dceb1",
   "metadata": {},
   "source": [
    "## Method 1: Sequential Download (One by One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0105652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_sequential(date, data_dir, base_url):\n",
    "    \"\"\"Download a single CHIRPS V3 file.\"\"\"\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    filename = f'chirps-v3.0.rnl.{year}.{month:02d}.{day:02d}.tif'\n",
    "    filepath = Path(data_dir) / filename\n",
    "    url = f'{base_url}/{year}/{filename}'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=300)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        return True, filename\n",
    "    except Exception as e:\n",
    "        return False, f\"{filename}: {str(e)}\"\n",
    "\n",
    "def download_all_sequential(dates, data_dir, base_url):\n",
    "    \"\"\"Download all files sequentially.\"\"\"\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    downloaded = []\n",
    "    failed = []\n",
    "    \n",
    "    for i, date in enumerate(dates, 1):\n",
    "        success, result = download_file_sequential(date, data_dir, base_url)\n",
    "        \n",
    "        if success:\n",
    "            downloaded.append(result)\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Downloaded {i}/{len(dates)} files...\")\n",
    "        else:\n",
    "            failed.append(result)\n",
    "    \n",
    "    return {'downloaded': len(downloaded), 'failed': len(failed), 'failed_files': failed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aebe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Sequential Download...\n",
      "Downloading 366 files one by one...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clean directory if exists\n",
    "if Path(DATA_DIR_SEQUENTIAL).exists():\n",
    "    shutil.rmtree(DATA_DIR_SEQUENTIAL)\n",
    "\n",
    "print(\"Starting Sequential Download...\")\n",
    "print(f\"Downloading {len(all_dates)} files one by one...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_sequential = download_all_sequential(all_dates, DATA_DIR_SEQUENTIAL, CHIRPS_V3_BASE_URL)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sequential Download Complete!\")\n",
    "print(f\"Downloaded: {result_sequential['downloaded']} files\")\n",
    "print(f\"Failed: {result_sequential['failed']} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f356dff6",
   "metadata": {},
   "source": [
    "## Method 2: Multithreaded Download (Concurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_threaded(date, data_dir, base_url):\n",
    "    \"\"\"Download a single file (thread-safe version).\"\"\"\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    filename = f'chirps-v3.0.rnl.{year}.{month:02d}.{day:02d}.tif'\n",
    "    filepath = Path(data_dir) / filename\n",
    "    url = f'{base_url}/{year}/{filename}'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=300)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        return True, filename\n",
    "    except Exception as e:\n",
    "        return False, f\"{filename}: {str(e)}\"\n",
    "\n",
    "def download_all_multithreaded(dates, data_dir, base_url, max_workers=10):\n",
    "    \"\"\"Download all files using multithreading.\"\"\"\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    downloaded = []\n",
    "    failed = []\n",
    "    completed = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all download tasks\n",
    "        future_to_date = {\n",
    "            executor.submit(download_file_threaded, date, data_dir, base_url): date \n",
    "            for date in dates\n",
    "        }\n",
    "        \n",
    "        # Process completed downloads\n",
    "        for future in as_completed(future_to_date):\n",
    "            completed += 1\n",
    "            success, result = future.result()\n",
    "            \n",
    "            if success:\n",
    "                downloaded.append(result)\n",
    "            else:\n",
    "                failed.append(result)\n",
    "            \n",
    "            if completed % 50 == 0:\n",
    "                print(f\"Downloaded {completed}/{len(dates)} files...\")\n",
    "    \n",
    "    return {'downloaded': len(downloaded), 'failed': len(failed), 'failed_files': failed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf86ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean directory if exists\n",
    "if Path(DATA_DIR_MULTITHREADED).exists():\n",
    "    shutil.rmtree(DATA_DIR_MULTITHREADED)\n",
    "\n",
    "print(\"Starting Multithreaded Download...\")\n",
    "print(f\"Downloading {len(all_dates)} files with {MAX_WORKERS} concurrent threads...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_multithreaded = download_all_multithreaded(\n",
    "    all_dates, \n",
    "    DATA_DIR_MULTITHREADED, \n",
    "    CHIRPS_V3_BASE_URL, \n",
    "    MAX_WORKERS\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Multithreaded Download Complete!\")\n",
    "print(f\"Downloaded: {result_multithreaded['downloaded']} files\")\n",
    "print(f\"Failed: {result_multithreaded['failed']} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582af96c",
   "metadata": {},
   "source": [
    "## Performance Summary\n",
    "\n",
    "Check the execution time shown by Jupyter for each download cell above.\n",
    "\n",
    "**Expected Results:**\n",
    "- Sequential: Downloads files one at a time (slower)\n",
    "- Multithreaded: Downloads multiple files simultaneously (significantly faster)\n",
    "\n",
    "**Optimization Recommendations:**\n",
    "1. **ThreadPoolExecutor** (implemented above) - Best for I/O-bound tasks like downloading\n",
    "2. **Adjust MAX_WORKERS** - Try values between 5-20 based on network bandwidth\n",
    "3. **Connection pooling** - Use `requests.Session()` to reuse connections\n",
    "4. **Async/await** - Use `aiohttp` for even better performance with many files\n",
    "\n",
    "The multithreaded approach should be 5-10x faster depending on network conditions and server limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701eecd",
   "metadata": {},
   "source": [
    "## Method 3: Async Download (Most Optimized) - OPTIONAL\n",
    "\n",
    "Using `aiohttp` with async/await for maximum performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install aiohttp if needed\n",
    "# !pip install aiohttp\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "DATA_DIR_ASYNC = './chirps_v3_async'\n",
    "\n",
    "async def download_file_async(session, date, data_dir, base_url):\n",
    "    \"\"\"Download a single file asynchronously.\"\"\"\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    filename = f'chirps-v3.0.rnl.{year}.{month:02d}.{day:02d}.tif'\n",
    "    filepath = Path(data_dir) / filename\n",
    "    url = f'{base_url}/{year}/{filename}'\n",
    "    \n",
    "    try:\n",
    "        async with session.get(url, timeout=300) as response:\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                async for chunk in response.content.iter_chunked(8192):\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        return True, filename\n",
    "    except Exception as e:\n",
    "        return False, f\"{filename}: {str(e)}\"\n",
    "\n",
    "async def download_all_async(dates, data_dir, base_url, max_concurrent=20):\n",
    "    \"\"\"Download all files asynchronously.\"\"\"\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    downloaded = []\n",
    "    failed = []\n",
    "    completed = 0\n",
    "    \n",
    "    # Create semaphore to limit concurrent downloads\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def download_with_semaphore(session, date):\n",
    "        async with semaphore:\n",
    "            return await download_file_async(session, date, data_dir, base_url)\n",
    "    \n",
    "    # Create session with connection pooling\n",
    "    connector = aiohttp.TCPConnector(limit=max_concurrent)\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        tasks = [download_with_semaphore(session, date) for date in dates]\n",
    "        \n",
    "        for i, coro in enumerate(asyncio.as_completed(tasks), 1):\n",
    "            success, result = await coro\n",
    "            completed += 1\n",
    "            \n",
    "            if success:\n",
    "                downloaded.append(result)\n",
    "            else:\n",
    "                failed.append(result)\n",
    "            \n",
    "            if completed % 50 == 0:\n",
    "                print(f\"Downloaded {completed}/{len(dates)} files...\")\n",
    "    \n",
    "    return {'downloaded': len(downloaded), 'failed': len(failed), 'failed_files': failed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3185bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean directory if exists\n",
    "if Path(DATA_DIR_ASYNC).exists():\n",
    "    shutil.rmtree(DATA_DIR_ASYNC)\n",
    "\n",
    "print(\"Starting Async Download...\")\n",
    "print(f\"Downloading {len(all_dates)} files with async/await (max 20 concurrent)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run async download\n",
    "result_async = await download_all_async(\n",
    "    all_dates, \n",
    "    DATA_DIR_ASYNC, \n",
    "    CHIRPS_V3_BASE_URL,\n",
    "    max_concurrent=20\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Async Download Complete!\")\n",
    "print(f\"Downloaded: {result_async['downloaded']} files\")\n",
    "print(f\"Failed: {result_async['failed']} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896e851",
   "metadata": {},
   "source": [
    "## Final Comparison\n",
    "\n",
    "Compare the execution times shown above for each method:\n",
    "\n",
    "| Method | Description | Expected Performance |\n",
    "|--------|-------------|---------------------|\n",
    "| **Sequential** | Downloads one file at a time | Baseline (slowest) |\n",
    "| **Multithreaded** | 10 concurrent threads | 5-10x faster |\n",
    "| **Async** | 20 concurrent connections | 8-15x faster |\n",
    "\n",
    "**Why Multithreading/Async is Faster:**\n",
    "- Network I/O is the bottleneck, not CPU\n",
    "- While waiting for one file, other downloads proceed\n",
    "- Better utilization of network bandwidth\n",
    "- Reduced idle time between requests\n",
    "\n",
    "**Note:** Actual speedup depends on:\n",
    "- Server rate limits\n",
    "- Network bandwidth\n",
    "- Latency to server\n",
    "- File sizes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
